{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pytorch-lightning --quiet\n",
    "!pip install transformers --quiet\n",
    "!pip install datasets --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model and Data Sources por Cañete, J., Chaperon, G., Fuentes, R., Pérez, J., & Bustos, B. (2020). Spanish Pre-Trained BERT Model and Evaluation Data. Recuperado de https://arxiv.org/abs/2308.02976\n",
    "\n",
    "# Spanish Pre-Trained BERT Model and Evaluation Data\n",
    "\n",
    "@inproceedings{CaneteCFP2020,\n",
    "  title={Spanish Pre-Trained BERT Model and Evaluation Data},\n",
    "  author={Cañete, José and Chaperon, Gabriel and Fuentes, Rodrigo and Ho, Jou-Hui and Kang, Hojin and Pérez, Jorge},\n",
    "  booktitle={PML4DC at ICLR 2020},\n",
    "  year={2020}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from transformers import BertModel, AutoTokenizer, AdamW\n",
    "import datasets\n",
    "import torch.nn.functional as F\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "# %load_ext tensorboard\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "\n",
    "# Hyperparámetros del modelo básicos: modelo preentrenado de BERT en español y longitud máxima de secuencia\n",
    "BERT_MODEL_NAME=\"dccuchile/bert-base-spanish-wwm-cased\"\n",
    "MAX_LEN = 128\n",
    "\n",
    "class DataModule(pl.LightningDataModule):\n",
    "    def __init__(self, train_path, val_path, test_path, batch_size, tokenizer, max_length):\n",
    "        super().__init__()\n",
    "        self.train_path = train_path\n",
    "        self.val_path = val_path\n",
    "        self.test_path = test_path\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(BERT_MODEL_NAME)\n",
    "        self.batch_size = batch_size\n",
    "        self.max_length = max_length\n",
    "        self.tokenizer = tokenizer\n",
    "        self.stop_words = set(stopwords.words('spanish'))  # Asumiendo que los textos están en español\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        '''\n",
    "        la funcion preprocess_text recibe un texto y realiza las siguientes operaciones:\n",
    "        - Tokeniza el texto\n",
    "        - Elimina stopwords\n",
    "        - Une los tokens filtrados en un solo texto\n",
    "        parámetros:\n",
    "        - text: texto a preprocesar\n",
    "        return:\n",
    "        - preprocessed_text: texto preprocesado\n",
    "        '''\n",
    "        # Tokenización\n",
    "        tokens = word_tokenize(text)\n",
    "        # Eliminación de stopwords\n",
    "        filtered_tokens = [token for token in tokens if token not in self.stop_words]\n",
    "        # Unir los tokens filtrados en un solo texto\n",
    "        preprocessed_text = ' '.join(filtered_tokens)\n",
    "        \n",
    "        return preprocessed_text\n",
    "\n",
    "    def load_dataset(self, path):\n",
    "        '''\n",
    "        la función load_dataset recibe la ruta de un archivo de texto y carga los datos en un DataFrame de pandas.\n",
    "        Luego, aplica el preprocesamiento de datos y aplica la funcion encode a la columna 'text' del DataFrame. Finalmente,\n",
    "        convierte los labels a tensores y depués el DataFrame en un objeto de tipo Dataset de la librería datasets compatible con torch\n",
    "        parámetros:\n",
    "        - path: ruta del archivo de texto\n",
    "        return:\n",
    "        - dataset: objeto de tipo Dataset de la librería datasets compatible con torch\n",
    "        '''\n",
    "        df = pd.read_csv(path, sep='\\t', header=None, names=['cat', 'text'])\n",
    "        df['labels'] = df.cat.map({0: 0, 1: 1})\n",
    "        df = df[['text', 'labels']]\n",
    "        df['labels'] = df['labels'].astype(np.int64)\n",
    "        df['text'] = df['text'].apply(self.preprocess_text)\n",
    "        df['labels'] = torch.tensor(df['labels'].values)\n",
    "        dataset = datasets.Dataset.from_pandas(df)\n",
    "        dataset = dataset.map(lambda examples: self.encode(examples), batched=True)\n",
    "        dataset.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'])\n",
    "        \n",
    "        return dataset\n",
    "\n",
    "    def encode(self, examples):\n",
    "        '''\n",
    "        la función encode recibe una columna de texto y aplica la tokenización de BERT a cada texto aplicando una serie de transformaciones y parámetros para optimizar la carga de datos\n",
    "        parámetros:\n",
    "        - examples: columna de texto a tokenizar\n",
    "        return:\n",
    "        - tokenized_text: texto tokenizado\n",
    "        '''\n",
    "        return self.tokenizer(examples['text'], add_special_tokens=True, truncation=True, padding='max_length', max_length=MAX_LEN, return_attention_mask=True, return_tensors='pt' )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        '''\n",
    "        la función train_dataloader carga los datos de entrenamiento y los convierte en un DataLoader de torch\n",
    "        return:\n",
    "        - DataLoader de torch con los datos de entrenamiento        \n",
    "        '''\n",
    "        dataset = self.load_dataset(self.train_path)\n",
    "        return torch.utils.data.DataLoader(dataset,\n",
    "                                           batch_size=self.batch_size,\n",
    "                                           shuffle=True,\n",
    "                                           num_workers=4,\n",
    "                                           persistent_workers=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        '''\n",
    "        la función val_dataloader carga los datos de validación y los convierte en un DataLoader de torch\n",
    "        return:\n",
    "        - DataLoader de torch con los datos de validación\n",
    "        '''\n",
    "        dataset = self.load_dataset(self.val_path)\n",
    "        return torch.utils.data.DataLoader(dataset,\n",
    "                                           batch_size=self.batch_size,\n",
    "                                           shuffle=False,\n",
    "                                           num_workers=4,\n",
    "                                           persistent_workers=True)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        ''''\n",
    "        la funcion test_dataloader carga los datos de test y los convierte en un DataLoader de torch\n",
    "        return:\n",
    "        - DataLoader de torch con los datos de test\n",
    "        '''\n",
    "        dataset = self.load_dataset(self.test_path)\n",
    "        return torch.utils.data.DataLoader(dataset,\n",
    "                                           batch_size=self.batch_size,\n",
    "                                           shuffle=False,\n",
    "                                           num_workers=4,\n",
    "                                           persistent_workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "HP_LEARNING_RATE = hp.HParam('learning_rate', hp.RealInterval(1e-5, 2e-5))\n",
    "HP_DROPOUT = hp.HParam('dropout_prob', hp.RealInterval(0.3, 0.5))\n",
    "HP_BATCH_SIZE = hp.HParam('batch_size', hp.Discrete([16, 32]))\n",
    "METRIC_ACCURACY = 'accuracy'\n",
    "hparams = {\n",
    "    HP_LEARNING_RATE: 2e-5,\n",
    "    HP_DROPOUT: 0.3,\n",
    "    HP_BATCH_SIZE: 16,\n",
    "}\n",
    "\n",
    "with tf.summary.create_file_writer('C:\\\\TFG\\\\Hyperparameters').as_default():\n",
    "    hp.hparams_config(\n",
    "        hparams=[HP_LEARNING_RATE, HP_DROPOUT, HP_BATCH_SIZE],\n",
    "        metrics=[hp.Metric(METRIC_ACCURACY, display_name='Accuracy')],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertSentimentClassifier(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super(BertSentimentClassifier, self).__init__()\n",
    "        self.save_hyperparameters(hparams)\n",
    "        self.learning_rate = torch.tensor(self.hparams[HP_LEARNING_RATE])\n",
    "        self.batch_size = torch.tensor(self.hparams[HP_BATCH_SIZE])\n",
    "        self._frozen = False\n",
    "        # loss function\n",
    "        self.criterion = torch.nn.CrossEntropyLoss()\n",
    "        # bert model\n",
    "        self.bert = BertModel.from_pretrained(BERT_MODEL_NAME)\n",
    "        self.dropout = torch.nn.Dropout(self.hparams[HP_DROPOUT])\n",
    "        self.fc = torch.nn.Linear(self.bert.config.hidden_size, 2)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        '''\n",
    "        la función configure_optimizers configura el optimizador AdamW con los hiperparámetros definidos en hparams para el modelo BERT\n",
    "        return:\n",
    "        - optimizer: optimizador AdamW\n",
    "        '''\n",
    "        optimizer = AdamW(self.parameters(), lr=self.hparams[HP_LEARNING_RATE])\n",
    "\n",
    "        return optimizer\n",
    "\n",
    "    def forward(self, batch):\n",
    "        '''\n",
    "        la función forward recibe un batch de datos y realiza el forward pass del modelo BERT, para esto aplica la tokenización de BERT a los datos de entrada y obtiene los logits y las probabilidades de las predicciones\n",
    "        parámetros:\n",
    "        - batch: batch de datos\n",
    "        return:\n",
    "        - logits: logits de las predicciones\n",
    "        '''\n",
    "        b_input_ids = batch['input_ids']\n",
    "        b_input_mask = batch['attention_mask']\n",
    "        b_token_type_ids = batch['token_type_ids']\n",
    "\n",
    "        outputs = self.bert(input_ids=b_input_ids,\n",
    "                            attention_mask=b_input_mask,\n",
    "                            token_type_ids= b_token_type_ids,\n",
    "                            return_dict=True)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.fc(pooled_output)\n",
    "        # Aplicar softmax para obtener probabilidades\n",
    "        probabilities = F.softmax(logits, dim=1)\n",
    "        return logits, probabilities\n",
    "\n",
    "    def on_train_start(self):\n",
    "        '''\n",
    "        la función on_train_start se ejecuta al inicio del entrenamiento y guarda los hiperparámetros en TensorBoard\n",
    "        '''\n",
    "        self.logger.log_hyperparams(self.hparams, {'hp/metric': 0})\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        '''\n",
    "        la función training_step recibe un batch de datos y realiza el forward pass del modelo BERT, calcula la loss y la precisión de las predicciones, además guarda los valores de loss y accuracy en TensorBoard\n",
    "        parámetros:\n",
    "        - batch: batch de datos\n",
    "        - batch_idx: índice del batch (se utiliza para implementar técnicas de entrenamiento como gradient accumulation)\n",
    "        return:\n",
    "        - loss: loss de las predicciones\n",
    "        '''\n",
    "        logits, probabilities = self.forward(batch)\n",
    "        loss = self.criterion(logits, batch['labels'])\n",
    "        # Calcular la precisión usando probabilidades\n",
    "        predictions = torch.argmax(probabilities, dim=1)\n",
    "        accuracy = (batch['labels'] == predictions).float().mean()\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log('train_accuracy', accuracy, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        \n",
    "        return {'loss': loss, 'accuracy': accuracy}\n",
    "        \n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        '''\n",
    "        la función validation_step recibe un batch de datos y realiza el forward pass del modelo BERT, calcula la loss y la precisión de las predicciones, además guarda los valores de loss y accuracy en TensorBoard\n",
    "        parámetros:\n",
    "        - batch: batch de datos \n",
    "        - batch_idx: índice del batch\n",
    "        return:\n",
    "        - loss: loss de las predicciones\n",
    "        '''\n",
    "        logits, probabilities = self.forward(batch)\n",
    "        loss = self.criterion(logits, batch['labels'])\n",
    "        # Calcular la precisión usando probabilidades\n",
    "        predictions = torch.argmax(probabilities, dim=1)\n",
    "        accuracy = (batch['labels'] == predictions).float().mean()\n",
    "                \n",
    "        # Log the validation loss and accuracy for visualization in TensorBoard\n",
    "        self.log('val_loss', loss, on_epoch=True, prog_bar=True)\n",
    "        self.log('val_accuracy', accuracy, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "        # Return the metrics\n",
    "        return {'val_loss': loss, 'val_accuracy': accuracy}\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        '''\n",
    "        la función test_step recibe un batch de datos y realiza el forward pass del modelo BERT, calcula la loss y la precisión de las predicciones, además guarda los valores de loss y accuracy en TensorBoard\n",
    "        parámetros:\n",
    "        - batch: batch de datos\n",
    "        - batch_idx: índice del batch\n",
    "        return:\n",
    "        - loss: loss de las predicciones\n",
    "        '''\n",
    "        logits, probabilities = self.forward(batch)\n",
    "        loss = self.criterion(logits, batch['labels'])\n",
    "        # Calcular la precisión usando probabilidades\n",
    "        predictions = torch.argmax(probabilities, dim=1)\n",
    "        accuracy = (batch['labels'] == predictions).float().mean()\n",
    "        \n",
    "        # Log the test loss and accuracy for visualization in TensorBoard\n",
    "        self.log('test_loss', loss, on_epoch=True, prog_bar=True)\n",
    "        self.log('test_accuracy', accuracy, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "        # Return the metrics\n",
    "        return {'test_loss': loss, 'test_accuracy': accuracy}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cargar datos\n",
    "train_path = 'C:\\TFG\\RawData\\Catalonian independence corpus\\spanish_train.csv'\n",
    "val_path = 'C:\\TFG\\RawData\\Catalonian independence corpus\\spanish_val.csv'\n",
    "test_path = 'C:\\TFG\\RawData\\Catalonian independence corpus\\spanish_test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se transforman los datos en DataFrames de pandas\n",
    "train = pd.read_csv(train_path, delimiter='\\t', encoding='utf-8', on_bad_lines='skip')\n",
    "val = pd.read_csv(val_path, delimiter='\\t', encoding='utf-8', on_bad_lines='skip')\n",
    "test = pd.read_csv(test_path, delimiter='\\t', encoding='utf-8', on_bad_lines='skip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenar\n",
    "df = pd.concat([train, val, test], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id_str', 'TWEET', 'LABEL'], dtype='object')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# se observan las columnas\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LABEL\n",
       "AGAINST    4105\n",
       "FAVOR      4104\n",
       "NEUTRAL    1868\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# frecuencia de LABEL\n",
    "df['LABEL'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop id_str columna\n",
    "df = df.drop(columns=['id_str'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename LABEL a cat y TWEET a text\n",
    "df = df.rename(columns={'LABEL': 'cat', 'TWEET': 'text'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_to_labels(label):\n",
    "    '''\n",
    "    la funcion label_to_labels recibe una etiqueta y la convierte en un valor numérico según la siguiente relación:\n",
    "    parámetros:\n",
    "    - label: etiqueta a convertir\n",
    "    return:\n",
    "    - valor numérico de la etiqueta\n",
    "    '''\n",
    "    if label == 'NEUTRAL':\n",
    "        return 2\n",
    "    elif label == 'FAVOR':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "df['cat'] = df['cat'].apply(label_to_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se filtran las columnas del dataset\n",
    "df = df[['cat', 'text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cat\n",
       "0    4105\n",
       "1    4104\n",
       "2    1868\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# se observan las frecuencias por categorías\n",
    "df['cat'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se prescinde de la etiqueta \"Neutral\" dado que hemos comprobado que en su gran mayoría contiene textos que no están escritos en español. Sino en portugués, gallego, etc. Por lo tanto, se ha decidido trabajar con las etiquetas \"Positive\" y \"Negative\". Ya que puede perjudicar a la precisión del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cat\n",
       "0    4104\n",
       "1    4104\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1868 instancias aleatorias de clase Negativo\n",
    "independencia_df_es_negativo = df[df['cat'] == 0].sample(4104)\n",
    "# 1868 instancias aleatorias de clase Positivo\n",
    "independencia_df_es_positivo = df[df['cat'] == 1].sample(4104)\n",
    "# 1868 instancias aleatorias de clase Neutral\n",
    "independencia_df_es_neutral = df[df['cat'] == 2].sample(1868)\n",
    "# concatenar los subconjuntos\n",
    "independencia_df_es_balanced = pd.concat([independencia_df_es_negativo, independencia_df_es_positivo])\n",
    "independencia_df_es_balanced = independencia_df_es_balanced.sample(frac=1)\n",
    "# se muestra el resultado\n",
    "independencia_df_es_balanced['cat'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dividir en train, val y test\n",
    "train, test = train_test_split(independencia_df_es_balanced, test_size=0.2, random_state=1335)\n",
    "train, val = train_test_split(train, test_size=0.25, random_state=1335)\n",
    "# guardar en .txt\n",
    "train.to_csv('C:\\TFG\\DataProcessed\\independencia_train.txt', sep='\\t', index=False, header=False)\n",
    "val.to_csv('C:\\TFG\\DataProcessed\\independencia_val.txt', sep='\\t', index=False, header=False)\n",
    "test.to_csv('C:\\TFG\\DataProcessed\\independencia_test.txt', sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is CUDA available: True\n",
      "CUDA version: 12.1\n",
      "cuDNN version: 8801\n"
     ]
    }
   ],
   "source": [
    "print(\"Is CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "print(\"cuDNN version:\", torch.backends.cudnn.version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\anaconda3\\envs\\ChromaDB\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of BertModel were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "BATCH_SIZE = 16 \n",
    "NUM_EPOCHS = 2\n",
    "tokenizer = AutoTokenizer.from_pretrained(BERT_MODEL_NAME)\n",
    "torch.manual_seed(1335)\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "data = DataModule('C:\\TFG\\DataProcessed\\independencia_train.txt', 'C:\\TFG\\DataProcessed\\independencia_val.txt', 'C:\\TFG\\DataProcessed\\independencia_test.txt', BATCH_SIZE, tokenizer, MAX_LEN)\n",
    "logdir = \"C:\\TFG\\Models\\\\bert_logs\"\n",
    "logger = TensorBoardLogger(logdir, name=\"bert\")\n",
    "model = BertSentimentClassifier()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertSentimentClassifier(\n",
      "  (criterion): CrossEntropyLoss()\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(31002, 768, padding_idx=1)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (fc): Linear(in_features=768, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "c:\\Users\\Lenovo\\anaconda3\\envs\\ChromaDB\\lib\\site-packages\\transformers\\optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | criterion | CrossEntropyLoss | 0     \n",
      "1 | bert      | BertModel        | 109 M \n",
      "2 | dropout   | Dropout          | 0     \n",
      "3 | fc        | Linear           | 1.5 K \n",
      "-----------------------------------------------\n",
      "109 M     Trainable params\n",
      "0         Non-trainable params\n",
      "109 M     Total params\n",
      "439.410   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Map:   0%|          | 0/1642 [00:00<?, ? examples/s]\n",
      "Map:  61%|██████    | 1000/1642 [00:01<00:00, 997.11 examples/s]\n",
      "Map: 100%|██████████| 1642/1642 [00:01<00:00, 1226.88 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Map:   0%|          | 0/4924 [00:00<?, ? examples/s]\n",
      "Map:  20%|██        | 1000/4924 [00:00<00:01, 3067.22 examples/s]\n",
      "Map:  41%|████      | 2000/4924 [00:00<00:00, 3192.15 examples/s]\n",
      "Map:  61%|██████    | 3000/4924 [00:01<00:00, 2478.90 examples/s]\n",
      "Map:  81%|████████  | 4000/4924 [00:01<00:00, 2583.76 examples/s]\n",
      "Map: 100%|██████████| 4924/4924 [00:01<00:00, 2504.87 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 308/308 [00:52<00:00,  5.82it/s, v_num=46, train_loss_step=0.460, train_accuracy_step=0.750]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\n",
      "Validation:   0%|          | 0/103 [00:00<?, ?it/s]\n",
      "Validation DataLoader 0:   0%|          | 0/103 [00:00<?, ?it/s]\n",
      "Validation DataLoader 0:   1%|          | 1/103 [00:00<00:04, 21.93it/s]\n",
      "Validation DataLoader 0:   2%|▏         | 2/103 [00:00<00:04, 23.64it/s]\n",
      "Validation DataLoader 0:   3%|▎         | 3/103 [00:00<00:04, 22.44it/s]\n",
      "Validation DataLoader 0:   4%|▍         | 4/103 [00:00<00:05, 17.00it/s]\n",
      "Validation DataLoader 0:   5%|▍         | 5/103 [00:00<00:05, 17.34it/s]\n",
      "Validation DataLoader 0:   6%|▌         | 6/103 [00:00<00:05, 17.77it/s]\n",
      "Validation DataLoader 0:   7%|▋         | 7/103 [00:00<00:05, 16.80it/s]\n",
      "Validation DataLoader 0:   8%|▊         | 8/103 [00:00<00:05, 16.90it/s]\n",
      "Validation DataLoader 0:   9%|▊         | 9/103 [00:00<00:05, 17.08it/s]\n",
      "Validation DataLoader 0:  10%|▉         | 10/103 [00:00<00:05, 17.29it/s]\n",
      "Validation DataLoader 0:  11%|█         | 11/103 [00:00<00:05, 17.56it/s]\n",
      "Validation DataLoader 0:  12%|█▏        | 12/103 [00:00<00:05, 17.90it/s]\n",
      "Validation DataLoader 0:  13%|█▎        | 13/103 [00:00<00:05, 17.97it/s]\n",
      "Validation DataLoader 0:  14%|█▎        | 14/103 [00:00<00:04, 18.22it/s]\n",
      "Validation DataLoader 0:  15%|█▍        | 15/103 [00:00<00:04, 18.33it/s]\n",
      "Validation DataLoader 0:  16%|█▌        | 16/103 [00:00<00:04, 18.54it/s]\n",
      "Validation DataLoader 0:  17%|█▋        | 17/103 [00:00<00:04, 18.67it/s]\n",
      "Validation DataLoader 0:  17%|█▋        | 18/103 [00:00<00:04, 18.81it/s]\n",
      "Validation DataLoader 0:  18%|█▊        | 19/103 [00:01<00:04, 18.74it/s]\n",
      "Validation DataLoader 0:  19%|█▉        | 20/103 [00:01<00:04, 18.90it/s]\n",
      "Validation DataLoader 0:  20%|██        | 21/103 [00:01<00:04, 18.98it/s]\n",
      "Validation DataLoader 0:  21%|██▏       | 22/103 [00:01<00:04, 19.07it/s]\n",
      "Validation DataLoader 0:  22%|██▏       | 23/103 [00:01<00:04, 19.25it/s]\n",
      "Validation DataLoader 0:  23%|██▎       | 24/103 [00:01<00:04, 19.20it/s]\n",
      "Validation DataLoader 0:  24%|██▍       | 25/103 [00:01<00:04, 19.18it/s]\n",
      "Validation DataLoader 0:  25%|██▌       | 26/103 [00:01<00:04, 19.00it/s]\n",
      "Validation DataLoader 0:  26%|██▌       | 27/103 [00:01<00:03, 19.01it/s]\n",
      "Validation DataLoader 0:  27%|██▋       | 28/103 [00:01<00:03, 19.06it/s]\n",
      "Validation DataLoader 0:  28%|██▊       | 29/103 [00:01<00:03, 19.06it/s]\n",
      "Validation DataLoader 0:  29%|██▉       | 30/103 [00:01<00:03, 19.05it/s]\n",
      "Validation DataLoader 0:  30%|███       | 31/103 [00:01<00:03, 19.07it/s]\n",
      "Validation DataLoader 0:  31%|███       | 32/103 [00:01<00:03, 19.14it/s]\n",
      "Validation DataLoader 0:  32%|███▏      | 33/103 [00:01<00:03, 18.92it/s]\n",
      "Validation DataLoader 0:  33%|███▎      | 34/103 [00:01<00:03, 19.04it/s]\n",
      "Validation DataLoader 0:  34%|███▍      | 35/103 [00:01<00:03, 19.09it/s]\n",
      "Validation DataLoader 0:  35%|███▍      | 36/103 [00:01<00:03, 19.20it/s]\n",
      "Validation DataLoader 0:  36%|███▌      | 37/103 [00:01<00:03, 19.20it/s]\n",
      "Validation DataLoader 0:  37%|███▋      | 38/103 [00:01<00:03, 19.29it/s]\n",
      "Validation DataLoader 0:  38%|███▊      | 39/103 [00:02<00:03, 19.39it/s]\n",
      "Validation DataLoader 0:  39%|███▉      | 40/103 [00:02<00:03, 19.39it/s]\n",
      "Validation DataLoader 0:  40%|███▉      | 41/103 [00:02<00:03, 19.49it/s]\n",
      "Validation DataLoader 0:  41%|████      | 42/103 [00:02<00:03, 19.60it/s]\n",
      "Validation DataLoader 0:  42%|████▏     | 43/103 [00:02<00:03, 19.77it/s]\n",
      "Validation DataLoader 0:  43%|████▎     | 44/103 [00:02<00:02, 19.87it/s]\n",
      "Validation DataLoader 0:  44%|████▎     | 45/103 [00:02<00:02, 19.97it/s]\n",
      "Validation DataLoader 0:  45%|████▍     | 46/103 [00:02<00:02, 20.12it/s]\n",
      "Validation DataLoader 0:  46%|████▌     | 47/103 [00:02<00:02, 20.23it/s]\n",
      "Validation DataLoader 0:  47%|████▋     | 48/103 [00:02<00:02, 20.33it/s]\n",
      "Validation DataLoader 0:  48%|████▊     | 49/103 [00:02<00:02, 20.46it/s]\n",
      "Validation DataLoader 0:  49%|████▊     | 50/103 [00:02<00:02, 20.54it/s]\n",
      "Validation DataLoader 0:  50%|████▉     | 51/103 [00:02<00:02, 20.67it/s]\n",
      "Validation DataLoader 0:  50%|█████     | 52/103 [00:02<00:02, 20.83it/s]\n",
      "Validation DataLoader 0:  51%|█████▏    | 53/103 [00:02<00:02, 20.92it/s]\n",
      "Validation DataLoader 0:  52%|█████▏    | 54/103 [00:02<00:02, 20.96it/s]\n",
      "Validation DataLoader 0:  53%|█████▎    | 55/103 [00:02<00:02, 21.07it/s]\n",
      "Validation DataLoader 0:  54%|█████▍    | 56/103 [00:02<00:02, 21.22it/s]\n",
      "Validation DataLoader 0:  55%|█████▌    | 57/103 [00:02<00:02, 21.31it/s]\n",
      "Validation DataLoader 0:  56%|█████▋    | 58/103 [00:02<00:02, 21.41it/s]\n",
      "Validation DataLoader 0:  57%|█████▋    | 59/103 [00:02<00:02, 21.48it/s]\n",
      "Validation DataLoader 0:  58%|█████▊    | 60/103 [00:02<00:02, 21.45it/s]\n",
      "Validation DataLoader 0:  59%|█████▉    | 61/103 [00:02<00:01, 21.54it/s]\n",
      "Validation DataLoader 0:  60%|██████    | 62/103 [00:02<00:01, 21.58it/s]\n",
      "Validation DataLoader 0:  61%|██████    | 63/103 [00:02<00:01, 21.67it/s]\n",
      "Validation DataLoader 0:  62%|██████▏   | 64/103 [00:02<00:01, 21.74it/s]\n",
      "Validation DataLoader 0:  63%|██████▎   | 65/103 [00:02<00:01, 21.85it/s]\n",
      "Validation DataLoader 0:  64%|██████▍   | 66/103 [00:03<00:01, 21.92it/s]\n",
      "Validation DataLoader 0:  65%|██████▌   | 67/103 [00:03<00:01, 21.92it/s]\n",
      "Validation DataLoader 0:  66%|██████▌   | 68/103 [00:03<00:01, 21.91it/s]\n",
      "Validation DataLoader 0:  67%|██████▋   | 69/103 [00:03<00:01, 21.95it/s]\n",
      "Validation DataLoader 0:  68%|██████▊   | 70/103 [00:03<00:01, 21.91it/s]\n",
      "Validation DataLoader 0:  69%|██████▉   | 71/103 [00:03<00:01, 21.91it/s]\n",
      "Validation DataLoader 0:  70%|██████▉   | 72/103 [00:03<00:01, 21.96it/s]\n",
      "Validation DataLoader 0:  71%|███████   | 73/103 [00:03<00:01, 22.04it/s]\n",
      "Validation DataLoader 0:  72%|███████▏  | 74/103 [00:03<00:01, 22.10it/s]\n",
      "Validation DataLoader 0:  73%|███████▎  | 75/103 [00:03<00:01, 22.09it/s]\n",
      "Validation DataLoader 0:  74%|███████▍  | 76/103 [00:03<00:01, 22.09it/s]\n",
      "Validation DataLoader 0:  75%|███████▍  | 77/103 [00:03<00:01, 22.16it/s]\n",
      "Validation DataLoader 0:  76%|███████▌  | 78/103 [00:03<00:01, 22.19it/s]\n",
      "Validation DataLoader 0:  77%|███████▋  | 79/103 [00:03<00:01, 22.25it/s]\n",
      "Validation DataLoader 0:  78%|███████▊  | 80/103 [00:03<00:01, 22.31it/s]\n",
      "Validation DataLoader 0:  79%|███████▊  | 81/103 [00:03<00:00, 22.27it/s]\n",
      "Validation DataLoader 0:  80%|███████▉  | 82/103 [00:03<00:00, 22.28it/s]\n",
      "Validation DataLoader 0:  81%|████████  | 83/103 [00:03<00:00, 22.27it/s]\n",
      "Validation DataLoader 0:  82%|████████▏ | 84/103 [00:03<00:00, 22.31it/s]\n",
      "Validation DataLoader 0:  83%|████████▎ | 85/103 [00:03<00:00, 22.35it/s]\n",
      "Validation DataLoader 0:  83%|████████▎ | 86/103 [00:03<00:00, 22.36it/s]\n",
      "Validation DataLoader 0:  84%|████████▍ | 87/103 [00:03<00:00, 22.24it/s]\n",
      "Validation DataLoader 0:  85%|████████▌ | 88/103 [00:03<00:00, 22.26it/s]\n",
      "Validation DataLoader 0:  86%|████████▋ | 89/103 [00:04<00:00, 22.25it/s]\n",
      "Validation DataLoader 0:  87%|████████▋ | 90/103 [00:04<00:00, 22.18it/s]\n",
      "Validation DataLoader 0:  88%|████████▊ | 91/103 [00:04<00:00, 22.21it/s]\n",
      "Validation DataLoader 0:  89%|████████▉ | 92/103 [00:04<00:00, 22.25it/s]\n",
      "Validation DataLoader 0:  90%|█████████ | 93/103 [00:04<00:00, 22.27it/s]\n",
      "Validation DataLoader 0:  91%|█████████▏| 94/103 [00:04<00:00, 22.21it/s]\n",
      "Validation DataLoader 0:  92%|█████████▏| 95/103 [00:04<00:00, 22.26it/s]\n",
      "Validation DataLoader 0:  93%|█████████▎| 96/103 [00:04<00:00, 22.21it/s]\n",
      "Validation DataLoader 0:  94%|█████████▍| 97/103 [00:04<00:00, 22.24it/s]\n",
      "Validation DataLoader 0:  95%|█████████▌| 98/103 [00:04<00:00, 22.24it/s]\n",
      "Validation DataLoader 0:  96%|█████████▌| 99/103 [00:04<00:00, 22.23it/s]\n",
      "Validation DataLoader 0:  97%|█████████▋| 100/103 [00:04<00:00, 22.28it/s]\n",
      "Validation DataLoader 0:  98%|█████████▊| 101/103 [00:04<00:00, 22.31it/s]\n",
      "Validation DataLoader 0:  99%|█████████▉| 102/103 [00:04<00:00, 22.33it/s]\n",
      "Validation DataLoader 0: 100%|██████████| 103/103 [00:04<00:00, 22.34it/s]\n",
      "Epoch 1: 100%|██████████| 308/308 [00:41<00:00,  7.36it/s, v_num=46, train_loss_step=0.724, train_accuracy_step=0.667, val_loss=0.535, val_accuracy=0.719, train_loss_epoch=0.626, train_accuracy_epoch=0.639] \n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\n",
      "Validation:   0%|          | 0/103 [00:00<?, ?it/s]\n",
      "Validation DataLoader 0:   0%|          | 0/103 [00:00<?, ?it/s]\n",
      "Validation DataLoader 0:   1%|          | 1/103 [00:00<00:03, 27.49it/s]\n",
      "Validation DataLoader 0:   2%|▏         | 2/103 [00:00<00:03, 25.26it/s]\n",
      "Validation DataLoader 0:   3%|▎         | 3/103 [00:00<00:04, 21.57it/s]\n",
      "Validation DataLoader 0:   4%|▍         | 4/103 [00:00<00:04, 20.88it/s]\n",
      "Validation DataLoader 0:   5%|▍         | 5/103 [00:00<00:04, 22.71it/s]\n",
      "Validation DataLoader 0:   6%|▌         | 6/103 [00:00<00:04, 24.12it/s]\n",
      "Validation DataLoader 0:   7%|▋         | 7/103 [00:00<00:03, 25.20it/s]\n",
      "Validation DataLoader 0:   8%|▊         | 8/103 [00:00<00:03, 24.58it/s]\n",
      "Validation DataLoader 0:   9%|▊         | 9/103 [00:00<00:03, 24.63it/s]\n",
      "Validation DataLoader 0:  10%|▉         | 10/103 [00:00<00:03, 25.20it/s]\n",
      "Validation DataLoader 0:  11%|█         | 11/103 [00:00<00:03, 25.95it/s]\n",
      "Validation DataLoader 0:  12%|█▏        | 12/103 [00:00<00:03, 26.45it/s]\n",
      "Validation DataLoader 0:  13%|█▎        | 13/103 [00:00<00:03, 26.30it/s]\n",
      "Validation DataLoader 0:  14%|█▎        | 14/103 [00:00<00:03, 26.72it/s]\n",
      "Validation DataLoader 0:  15%|█▍        | 15/103 [00:00<00:03, 27.14it/s]\n",
      "Validation DataLoader 0:  16%|█▌        | 16/103 [00:00<00:03, 27.57it/s]\n",
      "Validation DataLoader 0:  17%|█▋        | 17/103 [00:00<00:03, 27.62it/s]\n",
      "Validation DataLoader 0:  17%|█▋        | 18/103 [00:00<00:03, 27.56it/s]\n",
      "Validation DataLoader 0:  18%|█▊        | 19/103 [00:00<00:03, 27.56it/s]\n",
      "Validation DataLoader 0:  19%|█▉        | 20/103 [00:00<00:02, 27.79it/s]\n",
      "Validation DataLoader 0:  20%|██        | 21/103 [00:00<00:03, 26.97it/s]\n",
      "Validation DataLoader 0:  21%|██▏       | 22/103 [00:00<00:02, 27.24it/s]\n",
      "Validation DataLoader 0:  22%|██▏       | 23/103 [00:00<00:02, 27.21it/s]\n",
      "Validation DataLoader 0:  23%|██▎       | 24/103 [00:00<00:02, 26.93it/s]\n",
      "Validation DataLoader 0:  24%|██▍       | 25/103 [00:00<00:02, 26.78it/s]\n",
      "Validation DataLoader 0:  25%|██▌       | 26/103 [00:00<00:02, 27.06it/s]\n",
      "Validation DataLoader 0:  26%|██▌       | 27/103 [00:01<00:02, 26.98it/s]\n",
      "Validation DataLoader 0:  27%|██▋       | 28/103 [00:01<00:02, 27.20it/s]\n",
      "Validation DataLoader 0:  28%|██▊       | 29/103 [00:01<00:02, 27.20it/s]\n",
      "Validation DataLoader 0:  29%|██▉       | 30/103 [00:01<00:02, 27.21it/s]\n",
      "Validation DataLoader 0:  30%|███       | 31/103 [00:01<00:02, 27.35it/s]\n",
      "Validation DataLoader 0:  31%|███       | 32/103 [00:01<00:02, 27.41it/s]\n",
      "Validation DataLoader 0:  32%|███▏      | 33/103 [00:01<00:02, 27.41it/s]\n",
      "Validation DataLoader 0:  33%|███▎      | 34/103 [00:01<00:02, 27.26it/s]\n",
      "Validation DataLoader 0:  34%|███▍      | 35/103 [00:01<00:02, 27.34it/s]\n",
      "Validation DataLoader 0:  35%|███▍      | 36/103 [00:01<00:02, 27.25it/s]\n",
      "Validation DataLoader 0:  36%|███▌      | 37/103 [00:01<00:02, 26.96it/s]\n",
      "Validation DataLoader 0:  37%|███▋      | 38/103 [00:01<00:02, 26.91it/s]\n",
      "Validation DataLoader 0:  38%|███▊      | 39/103 [00:01<00:02, 26.93it/s]\n",
      "Validation DataLoader 0:  39%|███▉      | 40/103 [00:01<00:02, 27.00it/s]\n",
      "Validation DataLoader 0:  40%|███▉      | 41/103 [00:01<00:02, 27.03it/s]\n",
      "Validation DataLoader 0:  41%|████      | 42/103 [00:01<00:02, 26.93it/s]\n",
      "Validation DataLoader 0:  42%|████▏     | 43/103 [00:01<00:02, 26.72it/s]\n",
      "Validation DataLoader 0:  43%|████▎     | 44/103 [00:01<00:02, 26.67it/s]\n",
      "Validation DataLoader 0:  44%|████▎     | 45/103 [00:01<00:02, 26.54it/s]\n",
      "Validation DataLoader 0:  45%|████▍     | 46/103 [00:01<00:02, 26.58it/s]\n",
      "Validation DataLoader 0:  46%|████▌     | 47/103 [00:01<00:02, 26.55it/s]\n",
      "Validation DataLoader 0:  47%|████▋     | 48/103 [00:01<00:02, 26.52it/s]\n",
      "Validation DataLoader 0:  48%|████▊     | 49/103 [00:01<00:02, 26.27it/s]\n",
      "Validation DataLoader 0:  49%|████▊     | 50/103 [00:01<00:02, 26.33it/s]\n",
      "Validation DataLoader 0:  50%|████▉     | 51/103 [00:01<00:01, 26.30it/s]\n",
      "Validation DataLoader 0:  50%|█████     | 52/103 [00:01<00:01, 26.08it/s]\n",
      "Validation DataLoader 0:  51%|█████▏    | 53/103 [00:02<00:01, 25.93it/s]\n",
      "Validation DataLoader 0:  52%|█████▏    | 54/103 [00:02<00:01, 25.87it/s]\n",
      "Validation DataLoader 0:  53%|█████▎    | 55/103 [00:02<00:01, 25.72it/s]\n",
      "Validation DataLoader 0:  54%|█████▍    | 56/103 [00:02<00:01, 25.74it/s]\n",
      "Validation DataLoader 0:  55%|█████▌    | 57/103 [00:02<00:01, 25.64it/s]\n",
      "Validation DataLoader 0:  56%|█████▋    | 58/103 [00:02<00:01, 25.63it/s]\n",
      "Validation DataLoader 0:  57%|█████▋    | 59/103 [00:02<00:01, 25.65it/s]\n",
      "Validation DataLoader 0:  58%|█████▊    | 60/103 [00:02<00:01, 25.45it/s]\n",
      "Validation DataLoader 0:  59%|█████▉    | 61/103 [00:02<00:01, 25.37it/s]\n",
      "Validation DataLoader 0:  60%|██████    | 62/103 [00:02<00:01, 25.38it/s]\n",
      "Validation DataLoader 0:  61%|██████    | 63/103 [00:02<00:01, 25.33it/s]\n",
      "Validation DataLoader 0:  62%|██████▏   | 64/103 [00:02<00:01, 25.33it/s]\n",
      "Validation DataLoader 0:  63%|██████▎   | 65/103 [00:02<00:01, 25.33it/s]\n",
      "Validation DataLoader 0:  64%|██████▍   | 66/103 [00:02<00:01, 25.17it/s]\n",
      "Validation DataLoader 0:  65%|██████▌   | 67/103 [00:02<00:01, 24.99it/s]\n",
      "Validation DataLoader 0:  66%|██████▌   | 68/103 [00:02<00:01, 25.01it/s]\n",
      "Validation DataLoader 0:  67%|██████▋   | 69/103 [00:02<00:01, 25.03it/s]\n",
      "Validation DataLoader 0:  68%|██████▊   | 70/103 [00:02<00:01, 25.06it/s]\n",
      "Validation DataLoader 0:  69%|██████▉   | 71/103 [00:02<00:01, 25.04it/s]\n",
      "Validation DataLoader 0:  70%|██████▉   | 72/103 [00:02<00:01, 25.11it/s]\n",
      "Validation DataLoader 0:  71%|███████   | 73/103 [00:02<00:01, 25.11it/s]\n",
      "Validation DataLoader 0:  72%|███████▏  | 74/103 [00:02<00:01, 25.06it/s]\n",
      "Validation DataLoader 0:  73%|███████▎  | 75/103 [00:02<00:01, 25.08it/s]\n",
      "Validation DataLoader 0:  74%|███████▍  | 76/103 [00:03<00:01, 25.08it/s]\n",
      "Validation DataLoader 0:  75%|███████▍  | 77/103 [00:03<00:01, 25.05it/s]\n",
      "Validation DataLoader 0:  76%|███████▌  | 78/103 [00:03<00:00, 25.09it/s]\n",
      "Validation DataLoader 0:  77%|███████▋  | 79/103 [00:03<00:00, 25.14it/s]\n",
      "Validation DataLoader 0:  78%|███████▊  | 80/103 [00:03<00:00, 25.15it/s]\n",
      "Validation DataLoader 0:  79%|███████▊  | 81/103 [00:03<00:00, 25.06it/s]\n",
      "Validation DataLoader 0:  80%|███████▉  | 82/103 [00:03<00:00, 25.12it/s]\n",
      "Validation DataLoader 0:  81%|████████  | 83/103 [00:03<00:00, 25.10it/s]\n",
      "Validation DataLoader 0:  82%|████████▏ | 84/103 [00:03<00:00, 25.12it/s]\n",
      "Validation DataLoader 0:  83%|████████▎ | 85/103 [00:03<00:00, 25.09it/s]\n",
      "Validation DataLoader 0:  83%|████████▎ | 86/103 [00:03<00:00, 25.13it/s]\n",
      "Validation DataLoader 0:  84%|████████▍ | 87/103 [00:03<00:00, 25.19it/s]\n",
      "Validation DataLoader 0:  85%|████████▌ | 88/103 [00:03<00:00, 25.21it/s]\n",
      "Validation DataLoader 0:  86%|████████▋ | 89/103 [00:03<00:00, 25.23it/s]\n",
      "Validation DataLoader 0:  87%|████████▋ | 90/103 [00:03<00:00, 25.27it/s]\n",
      "Validation DataLoader 0:  88%|████████▊ | 91/103 [00:03<00:00, 25.18it/s]\n",
      "Validation DataLoader 0:  89%|████████▉ | 92/103 [00:03<00:00, 25.13it/s]\n",
      "Validation DataLoader 0:  90%|█████████ | 93/103 [00:03<00:00, 25.06it/s]\n",
      "Validation DataLoader 0:  91%|█████████▏| 94/103 [00:03<00:00, 25.06it/s]\n",
      "Validation DataLoader 0:  92%|█████████▏| 95/103 [00:03<00:00, 25.12it/s]\n",
      "Validation DataLoader 0:  93%|█████████▎| 96/103 [00:03<00:00, 25.13it/s]\n",
      "Validation DataLoader 0:  94%|█████████▍| 97/103 [00:03<00:00, 25.13it/s]\n",
      "Validation DataLoader 0:  95%|█████████▌| 98/103 [00:03<00:00, 25.15it/s]\n",
      "Validation DataLoader 0:  96%|█████████▌| 99/103 [00:03<00:00, 25.18it/s]\n",
      "Validation DataLoader 0:  97%|█████████▋| 100/103 [00:03<00:00, 25.14it/s]\n",
      "Validation DataLoader 0:  98%|█████████▊| 101/103 [00:04<00:00, 25.17it/s]\n",
      "Validation DataLoader 0:  99%|█████████▉| 102/103 [00:04<00:00, 25.22it/s]\n",
      "Validation DataLoader 0: 100%|██████████| 103/103 [00:04<00:00, 25.19it/s]\n",
      "Epoch 1: 100%|██████████| 308/308 [00:46<00:00,  6.69it/s, v_num=46, train_loss_step=0.724, train_accuracy_step=0.667, val_loss=0.505, val_accuracy=0.739, train_loss_epoch=0.417, train_accuracy_epoch=0.816]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=2` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 308/308 [00:55<00:00,  5.54it/s, v_num=46, train_loss_step=0.724, train_accuracy_step=0.667, val_loss=0.505, val_accuracy=0.739, train_loss_epoch=0.417, train_accuracy_epoch=0.816]\n"
     ]
    }
   ],
   "source": [
    "# Entrenar el modelo y medir el tiempo de ejecución\n",
    "start_time = time.time()\n",
    "beto_trainer = pl.Trainer(max_epochs = NUM_EPOCHS, logger=logger, accelerator=\"gpu\")\n",
    "beto_trainer.fit(model, datamodule=data)\n",
    "finish_time = time.time()\n",
    "beto_time = finish_time - start_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  15%|█▍        | 45/308 [10:49<1:03:14,  0.07it/s, v_num=45, train_loss_step=0.763, train_accuracy_step=0.500]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Map:   0%|          | 0/1642 [00:00<?, ? examples/s]\n",
      "Map:  61%|██████    | 1000/1642 [00:00<00:00, 2463.42 examples/s]\n",
      "Map: 100%|██████████| 1642/1642 [00:00<00:00, 2725.67 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 103/103 [00:08<00:00, 12.12it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test_accuracy       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7423873543739319     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.5025784969329834     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test_accuracy      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7423873543739319    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.5025784969329834    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'test_loss': 0.5025784969329834, 'test_accuracy': 0.7423873543739319}]\n"
     ]
    }
   ],
   "source": [
    "# Probar el modelo con el conjunto de test\n",
    "test_out = beto_trainer.test(model, datamodule=data)\n",
    "print(test_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('C:\\\\TFG\\\\Models\\\\bert\\\\independencia_tokenizer\\\\tokenizer_config.json',\n",
       " 'C:\\\\TFG\\\\Models\\\\bert\\\\independencia_tokenizer\\\\special_tokens_map.json',\n",
       " 'C:\\\\TFG\\\\Models\\\\bert\\\\independencia_tokenizer\\\\vocab.txt',\n",
       " 'C:\\\\TFG\\\\Models\\\\bert\\\\independencia_tokenizer\\\\added_tokens.json',\n",
       " 'C:\\\\TFG\\\\Models\\\\bert\\\\independencia_tokenizer\\\\tokenizer.json')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Guardar el data.tokenizer\n",
    "data.tokenizer.save_pretrained('C:\\\\TFG\\\\Models\\\\bert\\\\independencia_tokenizer')\n",
    "# cargar el data.tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('C:\\\\TFG\\\\Models\\\\bert\\\\independencia_tokenizer')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1])\n"
     ]
    }
   ],
   "source": [
    "# predecir la clase de un tweet con trainer.predict\n",
    "tweet = \"El 22/02 doy una conferencia de física cuántica:  Las escisiones del Uranio con isótopos en las tres fases ciclotrónicas de este elemento   Confieso que de física cuántica no tengo ni puta idea, pero viendo a @pablocasado_ hablar de democracia y DDHH,  me he venido arriba! ???\"\n",
    "tweet = data.tokenizer(tweet, add_special_tokens=True, truncation=True, padding='max_length', max_length=MAX_LEN, return_attention_mask=True, return_tensors='pt')\n",
    "tweet = {k: v.to(model.device) for k, v in tweet.items()}\n",
    "model.eval()\n",
    "output = model(tweet)\n",
    "predicted_class = torch.argmax(output[0], dim=1)\n",
    "print(predicted_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# guardar el modelo con pickle\n",
    "with open('C:\\\\TFG\\\\Models\\\\bert\\\\beto_model.pickle', 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\TFG\\\\Models\\\\bert\\\\beto_model.joblib']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# guardar el modelo con joblib\n",
    "import joblib\n",
    "\n",
    "joblib.dump(model, 'C:\\\\TFG\\\\Models\\\\bert\\\\beto_model.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# guardar el modelo con torch\n",
    "torch.save(model, 'C:\\\\TFG\\\\Models\\\\bert\\\\beto_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1])\n"
     ]
    }
   ],
   "source": [
    "# cargar el modelo\n",
    "model_path = 'C:\\\\TFG\\\\Models\\\\bert\\\\beto_model.pth'\n",
    "model = BertSentimentClassifier()\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "tweet = \"El 22/02 doy una conferencia de física cuántica:  Las escisiones del Uranio con isótopos en las tres fases ciclotrónicas de este elemento   Confieso que de física cuántica no tengo ni puta idea, pero viendo a @pablocasado_ hablar de democracia y DDHH,  me he venido arriba! ???\"\n",
    "tweet = data.tokenizer(tweet, add_special_tokens=True, truncation=True, padding='max_length', max_length=MAX_LEN, return_attention_mask=True, return_tensors='pt', return_token_type_ids=True)\n",
    "tweet = {k: v.to(model.device) for k, v in tweet.items()}\n",
    "model.eval()\n",
    "output = model(tweet)\n",
    "predicted_class = torch.argmax(output[0], dim=1)\n",
    "print(predicted_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>256.17769</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0\n",
       "0  256.17769"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# guardar beto_time como dataset\n",
    "beto_time = pd.DataFrame([beto_time])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cambiar nombre de columna a beto_time\n",
    "beto_time.columns = ['beto_time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# guardar beto_time\n",
    "beto_time.to_csv('C:\\\\TFG\\\\Metrics\\\\beto_time.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
