{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pytorch-wlightning --quiet\n",
    "%pip install transformers --quiet\n",
    "%pip install datasets --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autor\n",
    "Unidad de Minería de Textos (TeMU) en el Centro de Supercomputación de Barcelona ( bsc-temu@bsc.es )\n",
    "\n",
    "@inproceedings{armengol-estape-etal-2021-multilingual,\n",
    "    title = \"Are Multilingual Models the Best Choice for Moderately Under-resourced Languages? {A} Comprehensive Assessment for {C}atalan\",\n",
    "    author = \"Armengol-Estap{\\'e}, Jordi  and\n",
    "      Carrino, Casimiro Pio  and\n",
    "      Rodriguez-Penagos, Carlos  and\n",
    "      de Gibert Bonet, Ona  and\n",
    "      Armentano-Oller, Carme  and\n",
    "      Gonzalez-Agirre, Aitor  and\n",
    "      Melero, Maite  and\n",
    "      Villegas, Marta\",\n",
    "    booktitle = \"Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021\",\n",
    "    month = aug,\n",
    "    year = \"2021\",\n",
    "    address = \"Online\",\n",
    "    publisher = \"Association for Computational Linguistics\",\n",
    "    url = \"https://aclanthology.org/2021.findings-acl.437\",\n",
    "    doi = \"10.18653/v1/2021.findings-acl.437\",\n",
    "    pages = \"4933--4946\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from transformers import BertModel, AutoTokenizer, AdamW\n",
    "import datasets\n",
    "import torch.nn.functional as F\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "# cargar librería pickle para guardar el tokenizer\n",
    "import pickle\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataModule(pl.LightningDataModule):\n",
    "    def __init__(self, train_path, val_path, test_path, batch_size, tokenizer, max_length):\n",
    "        super().__init__()\n",
    "        self.train_path = train_path\n",
    "        self.val_path = val_path\n",
    "        self.test_path = test_path\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(BERT_MODEL_NAME)\n",
    "        self.batch_size = batch_size\n",
    "        self.max_length = max_length\n",
    "        self.tokenizer = tokenizer\n",
    "        self.stop_words = set(stopwords.words('catalan'))  # Asumiendo que los textos están en español\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        '''\n",
    "        la funcion preprocess_text recibe un texto y realiza las siguientes operaciones:\n",
    "        - Tokeniza el texto\n",
    "        - Elimina stopwords\n",
    "        - Une los tokens filtrados en un solo texto\n",
    "        parámetros:\n",
    "        - text: texto a preprocesar\n",
    "        return:\n",
    "        - preprocessed_text: texto preprocesado\n",
    "        '''\n",
    "        # Tokenización\n",
    "        tokens = word_tokenize(text)\n",
    "        # Eliminación de stopwords\n",
    "        filtered_tokens = [token for token in tokens if token not in self.stop_words]\n",
    "        # Unir los tokens filtrados en un solo texto\n",
    "        preprocessed_text = ' '.join(filtered_tokens)\n",
    "        \n",
    "        return preprocessed_text\n",
    "\n",
    "    def load_dataset(self, path):\n",
    "        '''\n",
    "        la función load_dataset recibe la ruta de un archivo de texto y carga los datos en un DataFrame de pandas.\n",
    "        Luego, aplica el preprocesamiento de datos y aplica la funcion encode a la columna 'text' del DataFrame. Finalmente,\n",
    "        convierte el DataFrame en un objeto de tipo Dataset de la librería datasets compatible con torch\n",
    "        parámetros:\n",
    "        - path: ruta del archivo de texto\n",
    "        return:\n",
    "        - dataset: objeto de tipo Dataset de la librería datasets compatible con torch\n",
    "        '''\n",
    "        df = pd.read_csv(path, sep='\\t', header=None, names=['cat', 'text'])\n",
    "        df['labels'] = df.cat.map({0: 0, 1: 1})\n",
    "        df = df[['text', 'labels']]\n",
    "        df['labels'] = df['labels'].astype(np.int64)\n",
    "        df['text'] = df['text'].apply(self.preprocess_text)\n",
    "        df['labels'] = torch.tensor(df['labels'].values)\n",
    "        dataset = datasets.Dataset.from_pandas(df)\n",
    "        dataset = dataset.map(lambda examples: self.encode(examples), batched=True)\n",
    "        dataset.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'])\n",
    "        \n",
    "        return dataset\n",
    "\n",
    "    def encode(self, examples):\n",
    "        '''\n",
    "        la función encode recibe una columna de texto y aplica la tokenización de BERT a cada texto aplicando una serie de transformaciones y parámetros para optimizar la carga de datos\n",
    "        parámetros:\n",
    "        - examples: columna de texto a tokenizar\n",
    "        return:\n",
    "        - tokenized_text: texto tokenizado\n",
    "        '''\n",
    "        return self.tokenizer(examples['text'], add_special_tokens=True, truncation=True, padding='max_length', max_length=MAX_LEN, return_attention_mask=True, return_tensors='pt', return_token_type_ids=True)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        '''\n",
    "        la función train_dataloader carga los datos de entrenamiento y los convierte en un DataLoader de torch\n",
    "        return:\n",
    "        - DataLoader de torch con los datos de entrenamiento        \n",
    "        '''\n",
    "        dataset = self.load_dataset(self.train_path)\n",
    "        return torch.utils.data.DataLoader(dataset, batch_size=self.batch_size, shuffle=True, num_workers=4, persistent_workers=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        '''\n",
    "        la función val_dataloader carga los datos de validación y los convierte en un DataLoader de torch\n",
    "        return:\n",
    "        - DataLoader de torch con los datos de validación\n",
    "        '''\n",
    "        dataset = self.load_dataset(self.val_path)\n",
    "        return torch.utils.data.DataLoader(dataset, batch_size=self.batch_size, shuffle=False, num_workers=4, persistent_workers=True)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        '''\n",
    "        la funcion test_dataloader carga los datos de test y los convierte en un DataLoader de torch\n",
    "        return:\n",
    "        - DataLoader de torch con los datos de test\n",
    "        '''\n",
    "        dataset = self.load_dataset(self.test_path)\n",
    "        return torch.utils.data.DataLoader(dataset, batch_size=self.batch_size, shuffle=False, num_workers=4, persistent_workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "HP_LEARNING_RATE = hp.HParam('learning_rate', hp.RealInterval(2e-5, 3e-5))\n",
    "HP_DROPOUT = hp.HParam('dropout_prob', hp.RealInterval(0.1, 0.3))\n",
    "HP_BATCH_SIZE = hp.HParam('batch_size', hp.Discrete([16, 32]))\n",
    "METRIC_ACCURACY = 'accuracy'\n",
    "hparams = {\n",
    "    HP_LEARNING_RATE: 2e-5,\n",
    "    HP_DROPOUT: 0.1,\n",
    "    HP_BATCH_SIZE: 16,\n",
    "}\n",
    "BERT_MODEL_NAME=\"projecte-aina/roberta-base-ca-v2\"\n",
    "MAX_LEN = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertSentimentClassifier(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super(BertSentimentClassifier, self).__init__()\n",
    "        self.save_hyperparameters(hparams)\n",
    "        self.learning_rate = torch.tensor(self.hparams[HP_LEARNING_RATE])\n",
    "        self.batch_size = torch.tensor(self.hparams[HP_BATCH_SIZE])\n",
    "        self._frozen = False\n",
    "        # loss function\n",
    "        self.criterion = torch.nn.CrossEntropyLoss()\n",
    "        # bert model\n",
    "        self.bert = BertModel.from_pretrained(BERT_MODEL_NAME)\n",
    "        self.dropout = torch.nn.Dropout(self.hparams[HP_DROPOUT])\n",
    "        self.fc = torch.nn.Linear(self.bert.config.hidden_size, 2)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        '''\n",
    "        la función configure_optimizers configura el optimizador AdamW con los hiperparámetros definidos en hparams para el modelo BERT\n",
    "        return:\n",
    "        - optimizer: optimizador AdamW\n",
    "        '''\n",
    "        optimizer = AdamW(self.parameters(), lr=self.hparams[HP_LEARNING_RATE])\n",
    "\n",
    "        return optimizer\n",
    "\n",
    "    def forward(self, batch):\n",
    "        '''\n",
    "        la función forward recibe un batch de datos y realiza el forward pass del modelo BERT, para esto aplica la tokenización de BERT a los datos de entrada y obtiene los logits y las probabilidades de las predicciones\n",
    "        parámetros:\n",
    "        - batch: batch de datos\n",
    "        return:\n",
    "        - logits: logits de las predicciones\n",
    "        '''\n",
    "        b_input_ids = batch['input_ids']\n",
    "        b_input_mask = batch['attention_mask']\n",
    "        b_token_type_ids = batch['token_type_ids']\n",
    "\n",
    "        outputs = self.bert(input_ids=b_input_ids,\n",
    "                            attention_mask=b_input_mask,\n",
    "                            token_type_ids= b_token_type_ids,\n",
    "                            return_dict=True)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.fc(pooled_output)\n",
    "        # Aplicar softmax para obtener probabilidades\n",
    "        probabilities = F.softmax(logits, dim=1)\n",
    "        return logits, probabilities\n",
    "\n",
    "    def on_train_start(self):\n",
    "        '''\n",
    "        la función on_train_start se ejecuta al inicio del entrenamiento y guarda los hiperparámetros en TensorBoard\n",
    "        '''\n",
    "        self.logger.log_hyperparams(self.hparams, {'hp/metric': 0})\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        '''\n",
    "        la función training_step recibe un batch de datos y realiza el forward pass del modelo BERT, calcula la loss y la precisión de las predicciones, además guarda los valores de loss y accuracy en TensorBoard\n",
    "        parámetros:\n",
    "        - batch: batch de datos\n",
    "        - batch_idx: índice del batch (se utiliza para implementar técnicas de entrenamiento como gradient accumulation)\n",
    "        return:\n",
    "        - loss: loss de las predicciones\n",
    "        '''\n",
    "        logits, probabilities = self.forward(batch)\n",
    "        loss = self.criterion(logits, batch['labels'])\n",
    "        # Calcular la precisión usando probabilidades\n",
    "        predictions = torch.argmax(probabilities, dim=1)\n",
    "        accuracy = (batch['labels'] == predictions).float().mean()\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log('train_accuracy', accuracy, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        \n",
    "        return {'loss': loss, 'accuracy': accuracy}\n",
    "        \n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        '''\n",
    "        la función validation_step recibe un batch de datos y realiza el forward pass del modelo BERT, calcula la loss y la precisión de las predicciones, además guarda los valores de loss y accuracy en TensorBoard\n",
    "        parámetros:\n",
    "        - batch: batch de datos \n",
    "        - batch_idx: índice del batch\n",
    "        return:\n",
    "        - loss: loss de las predicciones\n",
    "        '''\n",
    "        logits, probabilities = self.forward(batch)\n",
    "        loss = self.criterion(logits, batch['labels'])\n",
    "        # Calcular la precisión usando probabilidades\n",
    "        predictions = torch.argmax(probabilities, dim=1)\n",
    "        accuracy = (batch['labels'] == predictions).float().mean()\n",
    "                \n",
    "        # Log the validation loss and accuracy for visualization in TensorBoard\n",
    "        self.log('val_loss', loss, on_epoch=True, prog_bar=True)\n",
    "        self.log('val_accuracy', accuracy, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "        # Return the metrics\n",
    "        return {'val_loss': loss, 'val_accuracy': accuracy}\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        '''\n",
    "        la función test_step recibe un batch de datos y realiza el forward pass del modelo BERT, calcula la loss y la precisión de las predicciones, además guarda los valores de loss y accuracy en TensorBoard\n",
    "        parámetros:\n",
    "        - batch: batch de datos\n",
    "        - batch_idx: índice del batch\n",
    "        return:\n",
    "        - loss: loss de las predicciones\n",
    "        '''\n",
    "        logits, probabilities = self.forward(batch)\n",
    "        loss = self.criterion(logits, batch['labels'])\n",
    "        # Calcular la precisión usando probabilidades\n",
    "        predictions = torch.argmax(probabilities, dim=1)\n",
    "        accuracy = (batch['labels'] == predictions).float().mean()\n",
    "        \n",
    "        # Log the test loss and accuracy for visualization in TensorBoard\n",
    "        self.log('test_loss', loss, on_epoch=True, prog_bar=True)\n",
    "        self.log('test_accuracy', accuracy, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "        # Return the metrics\n",
    "        return {'test_loss': loss, 'test_accuracy': accuracy}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cargar datos\n",
    "train_path = 'C:\\TFG\\RawData\\Catalonian independence corpus\\catalan_train.csv'\n",
    "val_path = 'C:\\TFG\\RawData\\Catalonian independence corpus\\catalan_val.csv'\n",
    "test_path = 'C:\\TFG\\RawData\\Catalonian independence corpus\\catalan_test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cargar\n",
    "train = pd.read_csv(train_path, delimiter='\\t', encoding='utf-8', on_bad_lines='skip')\n",
    "val = pd.read_csv(val_path, delimiter='\\t', encoding='utf-8', on_bad_lines='skip')\n",
    "test = pd.read_csv(test_path, delimiter='\\t', encoding='utf-8', on_bad_lines='skip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenar\n",
    "df = pd.concat([train, val, test], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id_str', 'TWEET', 'LABEL'], dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LABEL\n",
       "AGAINST    3988\n",
       "FAVOR      3902\n",
       "NEUTRAL    2158\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# frecuencia de LABEL\n",
    "df['LABEL'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop id_str columna\n",
    "df = df.drop(columns=['id_str'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename LABEL a cat y TWEET a text\n",
    "df = df.rename(columns={'LABEL': 'cat', 'TWEET': 'text'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_to_labels(label):\n",
    "    '''\n",
    "    la funcion label_to_labels recibe una etiqueta y la convierte en un valor numérico según la siguiente relación:\n",
    "    parámetros:\n",
    "    - label: etiqueta a convertir\n",
    "    return:\n",
    "    - valor numérico de la etiqueta\n",
    "    '''\n",
    "    if label == 'NEUTRAL':\n",
    "        return 2\n",
    "    elif label == 'FAVOR':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "df['cat'] = df['cat'].apply(label_to_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['cat', 'text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cat\n",
       "0    3988\n",
       "1    3902\n",
       "2    2158\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['cat'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cat\n",
       "0    3902\n",
       "1    3902\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1868 instancias aleatorias de clase Negativo\n",
    "independencia_df_es_negativo = df[df['cat'] == 0].sample(3902)\n",
    "# 1868 instancias aleatorias de clase Positivo\n",
    "independencia_df_es_positivo = df[df['cat'] == 1].sample(3902)\n",
    "# 1868 instancias aleatorias de clase Neutral\n",
    "independencia_df_es_neutral = df[df['cat'] == 2].sample(2158)\n",
    "# concatenar los subconjuntos\n",
    "independencia_df_es_balanced = pd.concat([independencia_df_es_negativo, independencia_df_es_positivo])\n",
    "independencia_df_es_balanced = independencia_df_es_balanced.sample(frac=1)\n",
    "# se muestra el resultado\n",
    "independencia_df_es_balanced['cat'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dividir en train, val y test\n",
    "train, test = train_test_split(independencia_df_es_balanced, test_size=0.2, random_state=1335)\n",
    "train, val = train_test_split(train, test_size=0.25, random_state=1335)\n",
    "# guardar en .txt\n",
    "train.to_csv('C:\\TFG\\DataProcessed\\independencia_ca_train.txt', sep='\\t', index=False, header=False)\n",
    "val.to_csv('C:\\TFG\\DataProcessed\\independencia_ca_val.txt', sep='\\t', index=False, header=False)\n",
    "test.to_csv('C:\\TFG\\DataProcessed\\independencia_ca_test.txt', sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is CUDA available: True\n",
      "CUDA version: 12.1\n",
      "cuDNN version: 8801\n"
     ]
    }
   ],
   "source": [
    "print(\"Is CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "print(\"cuDNN version:\", torch.backends.cudnn.version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\anaconda3\\envs\\ChromaDB\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "You are using a model of type roberta to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of BertModel were not initialized from the model checkpoint at projecte-aina/roberta-base-ca-v2 and are newly initialized: ['embeddings.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.dense.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parametros\n",
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 2\n",
    "tokenizer = AutoTokenizer.from_pretrained(BERT_MODEL_NAME)\n",
    "torch.manual_seed(1335) # set seed to replicate results\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "data = DataModule('C:\\TFG\\DataProcessed\\independencia_ca_train.txt', 'C:\\TFG\\DataProcessed\\independencia_ca_val.txt', 'C:\\TFG\\DataProcessed\\independencia_ca_test.txt', BATCH_SIZE, tokenizer, MAX_LEN)\n",
    "logdir = \"C:\\TFG\\Models\\\\berta_logs\"\n",
    "logger = TensorBoardLogger(logdir, name=\"berta-ca\")\n",
    "model = BertSentimentClassifier()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertSentimentClassifier(\n",
      "  (criterion): CrossEntropyLoss()\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(50262, 768, padding_idx=1)\n",
      "      (position_embeddings): Embedding(514, 768)\n",
      "      (token_type_embeddings): Embedding(1, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (fc): Linear(in_features=768, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "c:\\Users\\Lenovo\\anaconda3\\envs\\ChromaDB\\lib\\site-packages\\transformers\\optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | criterion | CrossEntropyLoss | 0     \n",
      "1 | bert      | BertModel        | 124 M \n",
      "2 | dropout   | Dropout          | 0     \n",
      "3 | fc        | Linear           | 1.5 K \n",
      "-----------------------------------------------\n",
      "124 M     Trainable params\n",
      "0         Non-trainable params\n",
      "124 M     Total params\n",
      "498.579   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7034fd5796f47bb88bd50490c36565b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed9f8514d33d48db8daa9a5b7e1f431c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1561 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function DataModule.load_dataset.<locals>.<lambda> at 0x000001D31B33F820> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2444624d787f4d61b57f26e94e381baa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4682 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe8917ff6fef492185c7769ea4461da8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d679a609ebd4e4ba9d54dd8cbd7d17e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d882effc8b0b426cbbdc1d996ca02e7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=2` reached.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "trainer = pl.Trainer(max_epochs = NUM_EPOCHS, logger=logger, accelerator=\"gpu\")\n",
    "trainer.fit(model, datamodule=data)\n",
    "finish = time.time()\n",
    "roberta_time = finish - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8fadaf177284844894fee00233e545e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1561 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fd6d7a539d4402cac9ee973dee0d3f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test_accuracy       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7988469004631042     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.47888708114624023    </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test_accuracy      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7988469004631042    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.47888708114624023   \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'test_loss': 0.47888708114624023, 'test_accuracy': 0.7988469004631042}]\n"
     ]
    }
   ],
   "source": [
    "test_out = trainer.test(model, datamodule=data)\n",
    "print(test_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar el data.tokenizer\n",
    "data.tokenizer.save_pretrained('C:\\\\TFG\\\\Models\\\\roberta\\\\independencia_tokenizer')\n",
    "# cargar el data.tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('C:\\\\TFG\\\\Models\\\\roberta\\\\independencia_tokenizer')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1])\n"
     ]
    }
   ],
   "source": [
    "# predecir la clase de un tweet con trainer.predict\n",
    "tweet = \"Doncs aleshores ja sabem d'entrada que els 47 escons seran tots autonomistes. Tant si voteu com si no, perquè, suposo que no pretendreu fer creure que votar ERC, PDeCat o els seus succedanis és votar independentista, oi? https://t.co/rzAwQIOCWx\"\n",
    "tweet = data.tokenizer(tweet, add_special_tokens=True, truncation=True, padding='max_length', max_length=MAX_LEN, return_attention_mask=True, return_tensors='pt', return_token_type_ids=True)\n",
    "tweet = {k: v.to(model.device) for k, v in tweet.items()}\n",
    "model.eval()\n",
    "output = model(tweet)\n",
    "predicted_class = torch.argmax(output[0], dim=1)\n",
    "print(predicted_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# guardar el modelo con pickle\n",
    "with open('C:\\\\TFG\\\\Models\\\\roberta\\\\roberta_model.pickle', 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\TFG\\\\Models\\\\roberta\\\\roberta_model.joblib']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# guardar el modelo con joblib\n",
    "import joblib\n",
    "\n",
    "joblib.dump(model, 'C:\\\\TFG\\\\Models\\\\roberta\\\\roberta_model.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# guardar el modelo con torch\n",
    "torch.save(model, 'C:\\\\TFG\\\\Models\\\\roberta\\\\roberta_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# guardar roberta_time como dataset\n",
    "roberta_time = pd.DataFrame([roberta_time])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cambiar nombre de columna a roberta_time\n",
    "roberta_time.columns = ['roberta_time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# guardar beto_time\n",
    "roberta_time.to_csv('C:\\\\TFG\\\\Metrics\\\\roberta_time.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1])\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
